import os
import random
import re
import sys

DAMPING = 0.85
SAMPLES = 10000


def main():
    corpus = crawl('corpus0')
    ranks = sample_pagerank(corpus, DAMPING, SAMPLES)
    print(f"PageRank Results from Sampling (n = {SAMPLES})")
    for page in sorted(ranks):
        print(f"  {page}: {ranks[page]:.4f}")
    ranks = iterate_pagerank(corpus, DAMPING)
    print(f"PageRank Results from Iteration")
    for page in sorted(ranks):
        print(f"  {page}: {ranks[page]:.4f}")


def crawl(directory):
    """
    Parse a directory of HTML pages and check for links to other pages.
    Return a dictionary where each key is a page, and values are
    a list of all other pages in the corpus that are linked to by the page.
    """
    pages = dict()

    # Extract all links from HTML files
    for filename in os.listdir(directory):
        if not filename.endswith(".html"):
            continue
        with open(os.path.join(directory, filename)) as f:
            contents = f.read()
            links = re.findall(r"<a\s+(?:[^>]*?)href=\"([^\"]*)\"", contents)
            pages[filename] = set(links) - {filename}

    # Only include links to other pages in the corpus
    for filename in pages:
        pages[filename] = set(
            link for link in pages[filename]
            if link in pages
        )

    return pages


def transition_model(corpus, page, damping_factor):
    """
    Return a probability distribution over which page to visit next,
    given a current page.

    With probability `damping_factor`, choose a link at random
    linked to by `page`. With probability `1 - damping_factor`, choose
    a link at random chosen from all pages in the corpus.
    """
    distribution = {}

    # The amount of links coming from the page
    links = len(corpus[page])

    # If page has outgoing links
    if links:

        # For every link we give an additional value,
        # because with the probability of (1 - damping factor)
        # we chose at random between all of the links
        for link in corpus:

            distribution[link] = (1 - damping_factor) / len(corpus)

        # For every link to other sites from the site page,
        # we give additional value: damping factor divided
        # by number of links on the site page
        for link in corpus[page]:

            distribution[link] += damping_factor / links
    
    # If page doesn't have outgoing links
    else:

        # For every link we give value 1 / (total number of sites),
        # because we will have to chose the next site at random
        for link in corpus:

            distribution[link] = 1 / len(corpus)
    
    return distribution


def sample_pagerank(corpus, damping_factor, n):
    """
    Return PageRank values for each page by sampling `n` pages
    according to transition model, starting with a page at random.

    Return a dictionary where keys are page names, and values are
    their estimated PageRank value (a value between 0 and 1). All
    PageRank values should sum to 1.
    """
    distribution = {}

    for page in corpus:

        distribution[page] = 0
    
    # The first sample is generated by choosing from a corpus at random
    page = random.choice(list(corpus.keys()))

    # The next samples will be generated from the previous sample,
    # based on the previous sample transition model
    for i in range(n - 1):

        # Passing the sample into transition model
        current_distribution = transition_model(corpus, page, damping_factor)

        # Calculating new PageRank values for each page,
        # based on the previous PageRank Values
        for page in distribution:

            distribution[page] = (i * distribution[page] + current_distribution[page]) / (i + 1)
        
        # Choosing the next page in accordance with the PageRank model
        page = random.choices(list(distribution.keys()), distribution.values(), k=1)[0]

    return distribution

def iterate_pagerank(corpus, damping_factor):
    """
    Return PageRank values for each page by iteratively updating
    PageRank values until convergence.

    Return a dictionary where keys are page names, and values are
    their estimated PageRank value (a value between 0 and 1). All
    PageRank values should sum to 1.
    """
    ranks = {}
    threshold = 0.0005

    N = len(corpus)

    # Assigning each page a PageRank of 1 / (total number of pages)
    for key in corpus:

        ranks[key] = 1 / N
    
    # Repeatedly calculate new rank values
    # based on all of the current rank values
    while True:

        # Keeping the count on how many times the values of PageRank
        # changed by no more than (threshold)
        count = 0

        # Trying to count the probability than a random surfer
        # would end up on the site
        for key in corpus:

            # With the probability (1 - damping factor) the surfer
            # would chose the page at random and end up on the page
            new = (1 - damping_factor) / N

            sigma = 0

            # With probability damping factor the surfer would follow
            # a link from different page
            for page in corpus:
                
                # Count the number of links from the other
                # sites page to site key
                if key in corpus[page]:

                    num_links = len(corpus[page])

                    # Save the PageRank (chances that surfer would
                    # end up on the site) of the site from which
                    # there is a link, divide it by the total number
                    # of links on that site
                    sigma += ranks[page] / num_links
            
            sigma *= damping_factor

            new += sigma

            # Check the difference between the new and old value
            if abs(ranks[key] - new) < threshold:

                count += 1

            # Assign new value
            ranks[key] = new
        
        if count == N:

            break
    
    return ranks

if __name__ == "__main__":
    main()
